To generate the inputs, we first wrote a simple greedy solution for the problem that solved the given problem with the following algorithm: We first started off everyone in one room, then expanded the number of rooms by one if the current solution was incorrect. Afterwards, we took the worst offending student (defined by the heuristic total happiness generated - total stress caused) and moved them to the new room. We then iteratively called this on the next solution until we found an answer. After creating this simple algorithm, we confirmed that it was close to staff solution for the 10 input test and then started thinking of ways to break our solution. To do this, we thought of edge cases where the algorithm would choose the incorrect student to move. Thus, we built our 10 input and 20 input off the notion of breaking other people's greedy algorithms. This included toying with the stress and happiness values such that people who were greedying on one happiness/stress heuristic would fail. To confirm that these solutions were perfect, we ran a complete search on the 10 and 20 input with some pruning that didn't take too much time. However, for input 50, we weren't able to come up with a hard test so we pretty much self selected an answer and tried building the 50 inputs off that. However, our greedy solved this perfectly. As a result, we first worked on bettering our greedy solver using the techniques we learned in class to deal with NP completeness to solve the problem, then submitted the output of our algorithm on a randomly generated input. If given more time, we would have liked to think of our 50 input solution more and give a real one rather than a randomly generated one. Additionally, we would have liked to think a bit more about other approaches to this problem that weren't the greedy approach we used such as maybe a graph-based approach and try to break that as well with our input cases. We also ended up giving super cases in the sense that since 20 input contains 19! relationships, we weren't able to fully test algorithms as we gave most of the relationships values that would not screw with our edge cases.

We used our simple greedy algorithm and built off of it for our final algorithm that we used on the inputs. Some of the techniques we learned in class revolved around 3 concepts: backtracking, approximation, and heuristics. We tried to apply all of these. First, in our greedy algorithm, we put 3 heuristics: negative stress, happiness - stress, and full happiness to catch edge cases. These are techinically approximations, so for each of these greedy solutions we then tried to find local optimas. To do this, we swapped and inserted people into different rooms at a depth of at most 2. (AKA we didn't care about the validity of the swaps until the 2nd depth to ensure all swaps are tested). In a sense it was complete search starting at 3 different approximate solutions. Backtracking was used to conduct a swap, test it, then backtrack to save memory and allow us to test each possibility independently. 

We also tried to apply simulated annealing, but were unable to achieve effective results. If given time, we could try to figure out if we could get this to work to search for local optima of greater depth than we put in. Additionally, I think we could've found a way to ensure that identical solutions weren't checked. As depth increased, there were a lot of overlap on the outputs searched as well as since we searched 3 times, there was a lot of overlap there. This made our runtime a lot, so maybe if we could have a unique identifier like a bit string for each of these inputs we could then not waste time going down that path. As a result, we gated our depth of search at 2 where the higher the depth the better it is. So pretty much, in the future, we would have wished to be more efficient or find better heuristics that would have put us closer to the optimal.

Computational resources used were our own computers and the instructional machines. We didn't end up trusting instructional machines that much so we did most of the computation on our own computers when we went to sleep. AKA, we were scared that we would get kicked off because our runtimes were like upwards of a few hours. If given more time, we would've liked to figure out how to better use the instructional machines as well as other resources such as AWS to be more efficient in our testing. To test our work on the 50 input, we'd have to code each night, then let it run while we slept, and reconvene the next night to see the results of our changes. This was highly inefficient but was the best we could do. 

Overall, we really enjoyed this project and think we did pretty ok. I wish we would've spent more time searching for better greedy approximizations but I think what we ended up submitting was close to optimal.
